
<!-- saved from url=(0042)http://www.cs.cmu.edu/~pradeepr/convexopt/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><title>10716: ADVANCED MACHINE LEARNING</title>

  <link href="style.css" rel="stylesheet" type="text/css">
</head>

<body>
	<h1>ADVANCED MACHINE LEARNING</h1>
	<table border="0" cellpadding="1" cellspacing="1"> 
	<tbody><tr> <td> <b> 10-716, Spring 2019<b></b></b></td> </tr> 
	<tr> <td>POS 160, Tue &amp; Thurs 1:30PM - 2:50PM </td> </tr> 
</tbody></table> 

<br>
<table border="0" cellpadding="1" cellspacing="1"> 

<tbody><tr>
<td align="left" width="150" valign="top"> <b> Instructor</b> </td> 
<td>
<a href="http://www.cs.cmu.edu/~pradeepr">Pradeep Ravikumar</a> (pradeepr at cs dot cmu dot edu)<br><br>
</td>
</tr>

<tr>
<td align="left" width="150" valign="top"> <b> Teaching Assistants</b> </td> 
<td>
<a href="http://www.cs.cmu.edu/~pradeepr/716/">Ritika Mulagalapalli </a> (rmulagal at andrew dot cmu dot edu)<br>
	<a href="http://www.cs.cmu.edu/~pradeepr/716/">Leqi Liu</a> (leqil at cs dot cmu dot edu)<br>
	<a href="http://www.cs.cmu.edu/~pradeepr/716/">Boxiang (Shawn) Lyu</a> (blyu at andrew dot cmu dot edu)<br>
	<a href="http://www.cs.cmu.edu/~pradeepr/716/">Karthika Nair</a> (knair at andrew dot cmu dot edu)<br>
	<a href="http://www.cs.cmu.edu/~pradeepr/716/">Biswajit Paria</a> (bparia at cs dot cmu dot edu)<br>
 	<a href="http://www.cs.cmu.edu/~pradeepr/716/">Yao-Hung Tsai</a> (yaohungt at cs dot cmu dot edu)<br>
	
<br>
</td>
</tr>


<tr>
<td align="left" width="150" valign="top">  <b>Office Hours</b> </td> 
<td> 
	Pradeep Ravikumar: Thursday: 3:00pm - 4:00pm in GHC 8111<br>
	Shawn Lyu: Tuesdays 10:00am - 11:00am outside GHC 8009<br>
        Leqi Liu: Wednesdays 10:45am - 11:45am outside GHC 8009<br>
	Karthika Nair: Wednesdays 3:00pm - 4:00pm outside GHC 8009<br>
	Ritika Mulagalapalli: Mondays 11:00am - 12:00pm outside GHC 8009<br>
	Yao-Hung Tsai: Fridays 4:00pm - 5:00pm outside GHC 8009<br>
	Biswajit Paira: Thursdays 5:00pm - 6:00pm outside GHC 8009<br> 
</td>
</tr>

<tr><td> <br> </td></tr>

<tr> </tr>
<tr>	
<td align="left" width="150" valign="top">   <b><a name="courseDescription">Course Description</a></b></td>
<td>

	Advanced Machine Learning is a graduate level course introducing the theoretical foundations of modern machine learning, as well as advanced methods and frameworks used in modern machine learning. The course assumes that students have taken graduate level introductory courses in machine learning (Introduction to Machine Learning, 10-701 or 10-715), as well as Statistics (Intermediate Statistics, 36-700 or 36-705). The course treats both the art of designing good learning algorithms, as well as the science of analyzing an algorithm's computational and statistical properties and performance guarantees. We will cover theoretical foundation topics such as computational and statistical convergence rates, minimax estimation, and concentration of measure. We will also cover advanced machine learning methods such as nonparametric density estimation, nonparametric regression, and Bayesian estimation, as well as advanced frameworks such as privacy, causality, and stochastic learning algorithms.

</td>
</tr>



<tr> <td> <br></td> </tr>

<td align="left" width="150" valign="top">   <b>Grading </b> </td>
<td align="left" width="550"> 50% Homeworks, 15% Exam 1, 15% Exam 2, 20% Project
<br><br>
</td>
</tr>

<tr>
<td align="left" width="150" valign="top">   <b>Textbooks</b> </td> 
<td> Lectures are intended to be self-contained. The following references might be useful:
	<li>JB: Statistical Decision Theory and Bayesian Analysis, by James O. Berger </li>
	<li>MW: High-Dimensional Statistics: A Non-Asymptotic Viewpoint, by Martin J. Wainwright</li> 
	<li>BL: Prediction, Learning, and Games, by Nicolo Cesa-Bianchi, Gabor Lugosi</li>
	<li>W: All of Nonparametric Statistics, by Larry Wasserman</li>
	<li>AB: Computational Complexity: A Modern Approach, by Sanjeev Arora, Boaz Barak </li>
	<li>N: Introductory Lectures on Convex Optimization, by Yurii Nesterov</li>
<br>
</td></tr>



<tr> <td> <br></td> </tr>
<tr>	
<td align="left" width="150" valign="top">   <b><a name="details">Course details</a></b></td>
<td> <a href="Syllabus.pdf">Syllabus</a>.  <a href="http://www.cs.cmu.edu/~pradeepr/716/">Piazza</a>. <a href="http://www.cs.cmu.edu/~pradeepr/716/#homeworks">Homeworks</a>. <a href="http://www.cs.cmu.edu/~pradeepr/716/#project">Project.</a> <a href="https://docs.google.com/spreadsheets/d/1S1suk81goTP6jZLqWXveIBnPRYtBCTOQyh4NjRlfqyA/edit?usp=sharing">Scribe signup</a>. 
</tr>


<tr> <td> <br></td> </tr>
<tr>
<td align="left" width="150" valign="top">   <b>Tentative Schedule </b> </td>
<td align="left" width="850"> 

<table border="0" cellpadding="15" cellspacing="1"> 
		<tbody><tr class="top"> 
			<td> Date</td>
			<td> Topic </td> 
			<td> Readings </td>
			<td> Notes </td> 
		
		</tr> 

		<tr>
		<td colspan="6" class="gray"><b>Module: Statistical Decision Theory</b></td>
		</tr>
	
	
		<tr class="odd">
			<td> Jan 15 </td>
			<td> Decision Theory Principles and Paradigms <br/><a href="notes/lec1.pdf">Lecture Notes</a> </td>
			<td> JB Chap 1</td>
			<td> </td>
		</tr> 
		
		<tr class="odd">
			<td> Jan 17 </td>
			<td> Decision Theory Principles Contd. <br/><a href="notes/lec2.pdf">Lecture Notes</a> </td> 
			<td> JB Chap 4,2</td>
			<td> </td>
		</tr>
		
		<tr class="odd">
			<td> Jan 22 </td>
			<td> Bayesian Analysis <br/><a href="notes/lec3.pdf">Lecture Notes</a> </td> 
			<td> JB Chap 5</td>
			<td> </td>
		</tr>

		<tr class="odd">
			<td> Jan 24 </td>
			<td>Minimax Analysis <br/><a href="notes/lec4.pdf">Lecture Notes</a></td>
			<td> JB Chap 5</td>
			<td> <a href="hw/10716_HW1.pdf">HW1 out</a> </td>
		</tr>


		<tr>
		<td colspan="6" class="gray"><b>Module: Statistical Complexity</b></td>
		</tr>

		<tr class="odd">
			<td> Jan 29 </td>
			<td> Empirical Risk Minimization and Decision Theory, Tail Bounds <br/><a href="notes/lec5.pdf">Lecture Notes</a> </td>
			<td> MW Chap 2</td>
			<td> </td>
		</tr>
		
		<tr class="even">
			<td> Jan 31</td>
			<td> No class: Polar Vortex</td>
			<td> </td>
			<td> </td>

		</tr>


		<tr class="odd">
			<td> Feb 5 </td>
			<td> Tail Bounds Contd.<br/> <a href="notes/lec6.pdf">Lecture Notes</a> </td>
			<td> MW Chap 1,2</td>
			<a href="notes/lec5.pdf">Lecture Notes</a> <td> <a href="hw/10716_HW2.pdf">HW2 out</a>, HW1 due <a href="hw/hw1_soln.pdf">(Solutions)</a></td>
		</tr>

		<tr class="odd">
			<td> Feb 7 </td>
			<td> Tail Bounds Contd. <br/> <a href="notes/lec7.pdf">Lecture Notes</a> </td>
			<td> MW Chap 1,2</td>
			<td> </td>
		</tr>
	

		<tr class="odd">
			<td> Feb 12 </td>
			<td> Uniform Laws, Complexity Measures  <br/> <a href="notes/lec8.pdf">Lecture Notes</a> </td>
			<td> MW Chap 4</td>
			<td> </td>
		</tr>

		<tr class="odd">
			<td> Feb 14 </td>
			<td>  Uniform Laws, Complexity Measures Contd. <br/> <a href="notes/lec9.pdf">Lecture Notes</a> </td>
			<td> MW Chap 4</td>
			<td> </td>
		</tr>




		<tr class="even">
			<td> Feb 15 </td>
			<td></td>
			<td> </td>
			<td> HW2 due</td>
		</tr>

		<tr class="even">
			<td> Feb 19 </td>
			<td>Review Session</td>
			<td> </td>
			<td> </td>
		</tr>


		<tr class="even">
			<td> Feb 21 </td>
			<td>Test 1</td>
			<td> </td>
			<td> </td>
		</tr>
	
		<tr class="odd">
			<td> Feb 26 </td>
			<td> Sparse Linear Models  <br/> <a href="notes/lec10.pdf">Lecture Notes</a> </td>
			<td> MW Chap 7</td>
			<td> <a href="hw/10716_HW3.pdf">HW3 out</a>, Project Proposal Due</td>
		</tr>
		<tr class="odd">
			<td> Feb 28 </td>
			<td> Sparse Linear Models Contd. <br/> <a href="notes/lec11.pdf">Lecture Notes</a>  </td>
			<td> MW Chap 7</td>
			<td> </td>
		</tr>
	
		<tr class="odd">
			<td> Mar 5 </td>
			<td> Sparse Linear Models Contd. <br/> <a href="notes/lec13.pdf">Lecture Notes</a> </td>
			<td> MW Chap 7</td>
			<td> </td>
		</tr>




		<tr class="odd">
			<td> Mar 7 </td>
			<td>Lower bounds <br/><a href="notes/lec14.pdf">Lecture Notes</a></td>
			<td> MW Chap 15 </td>	
	<!--	<td> Papers: [<a href="https://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf">1</a>], [<a href="http://icml2008.cs.helsinki.fi/papers/266.pdf">2</a>]</td>
	--!>		
	<td> </td>
		</tr>





	
		<tr class="even">
			<td> Mar 8 </td>
			<td></td>
			<td> </td>
			<td> HW3 due</td>

		</tr>

		
		
		<tr class="even">
			<td> Mar 12 </td>
			<td>No Class: Spring Break</td>
			<td> </td>
			<td> </td>
		</tr>

		<tr class="even">
			<td> Mar 14 </td>
			<td>No Class: Spring Break</td>
			<td> </td>
			<td> </td>
		</tr>

		<tr class="odd">
			<td> Mar 19 </td>
			<td>Lower Bounds<br/><a href="notes/lec15.pdf">Lecture Notes</a></td>
			<td> MW Chap 15</td>
			<td> </td>
		</tr>
	
<tr>
		<td colspan="6" class="gray"><b>Module: Computational Complexity</b></td>
		</tr>



		<tr class="even">
			<td> Mar 22 </td>
			<td></td>
			<td> </td>
			<td> <a href="hw/10716_HW4.pdf">HW4 out</a> </td>

		</tr>



		

	<tr class="odd">
			<td> Mar 26 </td>
			<td>Optimization and Statistical Complexity<br/><a href="notes/lec16.pdf">Lecture Notes</a></td>
			<td> Papers: [<a href="https://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf">1</a>], [<a href="https://arxiv.org/abs/1804.01619">2</a>], [<a href= "http://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf">3</a>], [<a href="https://papers.nips.cc/paper/8260-connecting-optimization-and-regularization-paths.pdf">4</a>]</td>
			<td>  </td>
		</tr>





		<tr class="odd">
			<td> Mar 28 </td>
			<td>Optimization and Statistical Complexity Contd.</td>
			<td> Papers: [<a href="https://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf">1</a>], [<a href="https://arxiv.org/abs/1804.01619">2</a>], [<a href= "http://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf">3</a>],[<a href="https://papers.nips.cc/paper/8260-connecting-optimization-and-regularization-paths.pdf">4</a>]</td>
			<td> </td>
		</tr>

		<tr class="odd">
			<td> Apr 2 </td>
			<td>Intro to Computational Complexity, Stat. vs Complexity tradeoffs</td>
			<td>AB Chap 1,2<br><br> Papers: [<a href="http://users.cms.caltech.edu/~venkatc/cj_compstat_pnas13.pdf">1</a>], [<a href="https://www.cs.huji.ac.il/~shais/papers/ShalevShamirTromer12.pdf">2</a>]</td>
			<td> </td>
		</tr>
		
		<tr class="odd">
			<td> Apr 4 </td>
			<td>Stat. vs Complexity tradeoffs contd., Oracle Complexity</td>
			<td>Papers: [<a href="http://proceedings.mlr.press/v30/Berthet13.pdf">1</a>], [<a href="https://arxiv.org/pdf/1808.06996.pdf">2</a>]</td>
			<td> HW4 due, HW5 out</td>
		</tr>

		<tr>
		<td colspan="6" class="gray"><b>Module: Sequential Settings</b></td>
		</tr>
<tr class="odd">
			<td> Apr 9 </td>
			<td>Prediction with expert advice</td>
			<td>BL Chap 1, 2</td>
			<td>Project progress report due </td>
		</tr>

	
		<tr class="even">
			<td> Apr 11 </td>
			<td>No Class: Spring Carnival</td>
			<td> </td>
			<td> </td>
		</tr>	
		<tr class="odd">
			<td> Apr 16 </td>
			<td>Prediction with limited feedback</td>
			<td>BL Chap 6</td>
			<td> </td>
		</tr>
		
		<tr class="odd">
			<td> Apr 18 </td>
			<td>Prediction and games</td>
			<td>BL Chap 7</td>
			<td> HW5 due</td>
		</tr>



		<tr>
		<td colspan="6" class="gray"><b>Module: Nonparametric Estimation</b></td>
		</tr>


		<tr class="odd">
			<td> Apr 23 </td>
			<td>Nonparametric Regression: Linear, Local Smoothers</td>
			<td>W Chap 4,5</td>
			<td>  </td>
		</tr>


		
		
	
		
		<tr class="odd">
			<td> Apr 25 </td>
			<td>Nonparametric Regression: Orthogonal Functions</td>
			<td>W Chap 8,7</td>
			<td> </td>
		</tr>		
		

	
	<!--
		<tr class="odd">
			<td> Apr 25 </td>
			<td>Deep Density Estimation</td>
			<td> Papers: [<a href="http://proceedings.mlr.press/v80/inouye18a/inouye18a.pdf">1</a>]</td>

			<td> </td>
		</tr>
		
		<tr class="odd">
			<td> Apr 25 </td>
			<td>Nonparametric Mixture Models</td>
			<td>Papers: [<a href="https://www.cs.cmu.edu/~pradeepr/arxiv_npmix_v2.pdf">1</a>]</td>
			<td> </td>
		</tr>
-->
		
		<tr class="even">
			<td> Apr 30 </td>
			<td>Review Session</td>
			<td> </td>
			<td> </td>
		</tr>

		<tr class="even">
			<td> May 2 </td>
			<td>Test 2</td>
			<td> </td>
			<td> </td>
		</tr>


		<tr class="even">
			<td> May 7 </td>
			<td></td>
			<td> </td> 
			<td> Final Project Report Due</td>
		</tr>				
		
		
</tbody></table>
</td>
</tr>

<tr> <td> <br></td> </tr>
<tr>	
<td align="left" width="150" valign="top">   <b><a name="homeworks">Homeworks</a></b></td>
<td>

	There will be 5 homework assignments, approximately evenly spaced throughout the semester. The assignments will be posted on the course website, and on Piazza. We will use Gradescope for submitting, and grading assignments. You will get a late day quota of 10 days, which you can distribute among the five homeworks as you wish, subject to a maximum of 3 days per homework. Homeworks submitted after your late day quota will lose all points. The homework schedule is posted right at the beginning of the semester, so please plan in advance. We expect you to use the late day quota for conference deadlines and events of the like, so we cannot provide an additional extension for such cases. In the case of an emergency (sudden sickness, family problems, etc.), we can give you a reasonable extension. But we emphasize that this is reserved for true emergencies. 


</td>
</tr>



<tr> <td> <br></td> </tr>
<tr>	
<td align="left" width="150" valign="top">   <b><a name="project">Class Project</a></b></td>
<td>

	There will be a class project. You can form groups of up to 2 students. Further details can be found  <a href="ProjectInstructions.pdf">here</a>. 

</td>
</tr>



